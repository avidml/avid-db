{
    "data_type": "AVID",
    "data_version": "0.2",
    "metadata": {
        "vuln_id": "AVID-2023-V027"
    },
    "affects": {
        "developer": [
            "OpenAI"
        ],
        "deployer": [
            "OpenAI"
        ],
        "artifacts": [
            {
                "type": "System",
                "name": "ChatGPT"
            }
        ]
    },
    "problemtype": {
        "classof": "LLM Evaluation",
        "type": "Issue",
        "description": {
            "lang": "eng",
            "value": "ChatGPT generates false or incomplete references to scientific literature"
        }
    },
    "references": [
        {
            "type": "screenshot",
            "label": "Screenshot of example answer",
            "url": "../img/R00031.png"
        }
    ],
    "description": {
        "lang": "eng",
        "value": "When asked to recommend papers on explainability, privacy, adversarial ML, etc. ChatGPT recommends papers that (a) may not always exist, (b) mixes up correct and incorrect information, e.g. correct title but wrong authors, or (c) have incomplete information on authors."
    },
    "reports": [
        {
            "report_id": "AVID-2023-R0003",
            "type": "Issue",
            "name": "ChatGPT links wrong authors to papers"
        }
    ],
    "impact": {
        "avid": {
            "risk_domain": [
                "Ethics"
            ],
            "sep_view": [
                "E0402: Generative Misinformation"
            ],
            "lifecycle_view": [
                "L05: Evaluation",
                "L06: Deployment"
            ],
            "taxonomy_version": "0.2"
        }
    },
    "credit": [
        {
            "lang": "eng",
            "value": "Jaydeep Borkar, N/A"
        }
    ],
    "published_date": "2023-03-31",
    "last_modified_date": "2023-03-31"
}