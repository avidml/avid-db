{
    "data_type": "AVID",
    "data_version": "0.2",
    "metadata": {
        "vuln_id": "AVID-2023-V016"
    },
    "affects": {
        "developer": [],
        "deployer": [
            "MathGPT (https://mathgpt.streamlit.app/)"
        ],
        "artifacts": [
            {
                "type": "System",
                "name": "MathGPT (https://mathgpt.streamlit.app/)"
            }
        ]
    },
    "problemtype": {
        "classof": "ATLAS Case Study",
        "type": "Advisory",
        "description": {
            "lang": "eng",
            "value": "Achieving Code Execution in MathGPT via Prompt Injection"
        }
    },
    "references": null,
    "description": {
        "lang": "eng",
        "value": "The publicly available Streamlit application [MathGPT](https://mathgpt.streamlit.app/) uses GPT-3, a large language model (LLM), to answer user-generated math questions.\n\nRecent studies and experiments have shown that LLMs such as GPT-3 show poor performance when it comes to performing exact math directly[<sup>\\[1\\]</sup>][1][<sup>\\[2\\]</sup>][2]. However, they can produce more accurate answers when asked to generate executable code that solves the question at hand. In the MathGPT application, GPT-3 is used to convert the user's natural language question into Python code that is then executed. After computation, the executed code and the answer are displayed to the user.\n\nSome LLMs can be vulnerable to prompt injection attacks, where malicious user inputs cause the models to perform unexpected behavior[<sup>\\[3\\]</sup>][3][<sup>\\[4\\]</sup>][4].   In this incident, the actor explored several prompt-override avenues, producing code that eventually led to the actor gaining access to the application host system's environment variables and the application's GPT-3 API key, as well as executing a denial of service attack.  As a result, the actor could have exhausted the application's API query budget or brought down the application.\n\nAfter disclosing the attack vectors and their results to the MathGPT and Streamlit teams, the teams took steps to mitigate the vulnerabilities, filtering on select prompts and rotating the API key.\n\n[1]: https://arxiv.org/abs/2103.03874 \"Measuring Mathematical Problem Solving With the MATH Dataset\"\n[2]: https://arxiv.org/abs/2110.14168 \"Training Verifiers to Solve Math Word Problems\"\n[3]: https://lspace.swyx.io/p/reverse-prompt-eng \"Reverse Prompt Engineering for Fun and (no) Profit\"\n[4]: https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/ \"Exploring prompt-based attacks\""
    },
    "reports": null,
    "impact": {
        "avid": {
            "risk_domain": [
                "Security"
            ],
            "sep_view": [
                "S0403: Adversarial Example"
            ],
            "lifecycle_view": [
                "L06: Deployment"
            ],
            "taxonomy_version": "0.2"
        }
    },
    "credit": null,
    "published_date": "2023-03-30",
    "last_modified_date": "2023-03-30"
}