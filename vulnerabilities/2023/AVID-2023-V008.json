{
    "data_type": "AVID",
    "data_version": "0.2",
    "metadata": {
        "vuln_id": "AVID-2023-V008"
    },
    "affects": {
        "developer": [],
        "deployer": [
            "OpenAI GPT-2"
        ],
        "artifacts": [
            {
                "type": "System",
                "name": "OpenAI GPT-2"
            }
        ]
    },
    "problemtype": {
        "classof": "ATLAS Case Study",
        "type": "Advisory",
        "description": {
            "lang": "eng",
            "value": "GPT-2 Model Replication"
        }
    },
    "references": [
        {
            "type": "source",
            "label": "GPT-2 Model Replication",
            "url": "https://atlas.mitre.org/studies/AML.CS0007"
        },
        {
            "type": "source",
            "label": "Wired Article, \"OpenAI Said Its Code Was Risky. Two Grads Re-Created It Anyway\"",
            "url": "https://www.wired.com/story/dangerous-ai-open-source/"
        },
        {
            "type": "source",
            "label": "Medium BlogPost, \"OpenGPT-2: We Replicated GPT-2 Because You Can Too\"",
            "url": "https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc"
        }
    ],
    "description": {
        "lang": "eng",
        "value": "OpenAI built GPT-2, a language model capable of generating high quality text samples. Over concerns that GPT-2 could be used for malicious purposes such as impersonating others, or generating misleading news articles, fake social media content, or spam, OpenAI adopted a tiered release schedule. They initially released a smaller, less powerful version of GPT-2 along with a technical description of the approach, but held back the full trained model.\n\nBefore the full model was released by OpenAI, researchers at Brown University successfully replicated the model using information released by OpenAI and open source ML artifacts. This demonstrates that a bad actor with sufficient technical skill and compute resources could have replicated GPT-2 and used it for harmful goals before the AI Security community is prepared.\n"
    },
    "reports": null,
    "impact": {
        "avid": {
            "risk_domain": [
                "Security"
            ],
            "sep_view": [
                "S0502: Model theft"
            ],
            "lifecycle_view": [
                "L04: Model Development",
                "L06: Deployment"
            ],
            "taxonomy_version": "0.2"
        }
    },
    "credit": null,
    "published_date": "2023-03-31",
    "last_modified_date": "2023-03-31"
}