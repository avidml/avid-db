{
    "data_type": "AVID",
    "data_version": null,
    "metadata": null,
    "affects": {
        "developer": [],
        "deployer": [
            "Kaspersky's Antimalware ML Model"
        ],
        "artifacts": {
            "type": "system",
            "name": "Kaspersky's Antimalware ML Model"
        }
    },
    "problemtype": {
        "classof": "ATLAS Case Study",
        "type": "Advisory",
        "description": {
            "lang": "eng",
            "value": "Confusing Antimalware Neural Networks"
        }
    },
    "metrics": null,
    "references": [
        {
            "type": "source",
            "label": "Confusing Antimalware Neural Networks",
            "url": "https://atlas.mitre.org/studies/AML.CS0014"
        },
        {
            "type": "source",
            "label": "Article, \"How to confuse antimalware neural networks. Adversarial attacks and protection\"",
            "url": "https://securelist.com/how-to-confuse-antimalware-neural-networks-adversarial-attacks-and-protection/102949/"
        }
    ],
    "description": {
        "lang": "eng",
        "value": "Cloud storage and computations have become popular platforms for deploying ML malware detectors.\nIn such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.\nThe Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models.\n\nThey attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files."
    },
    "impact": null,
    "credit": null,
    "reported_date": "2021-06-23"
}